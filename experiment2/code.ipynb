
# Importing necessary libraries
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import make_moons

# Generating a synthetic moon-shaped dataset
samples = 1000
X, y = make_moons(
    samples,      # Number of samples
    noise=0.03,   # Adding some noise to make the data less linearly separable
    random_state=42  # For reproducibility
)
X.shape, y.shape  # Checking the shape of the features and labels

# Creating a pandas DataFrame for better visualization and manipulation of the data
moons = pd.DataFrame({
    "X0": X[:, 0],  # First feature
    "X1": X[:, 1],  # Second feature
    "labels": y     # Target variable (labels)
})
moons.head()  # Displaying the first few rows of the DataFrame

# Visualizing the dataset using a scatter plot with different colors for different classes
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)

# Splitting the dataset into training and testing sets (800 samples for training, 200 for testing)
x_train, y_train = X[:800], y[:800]
x_test, y_test = X[800:], y[800:]
x_train.shape, y_train.shape, x_test.shape, y_test.shape  # Verifying the shape of the split data

# Initializing a dictionary to store the results of the models
model_results = {}

# Setting the random seed for reproducibility
tf.random.set_seed(42)

# Defining the first model: A simple neural network with one hidden layer
model_1 = tf.keras.Sequential([
    tf.keras.layers.Dense(4, activation="relu"),  # Hidden layer with 4 neurons and ReLU activation
    tf.keras.layers.Dense(1, activation="sigmoid")  # Output layer with sigmoid activation for binary classification
])

# Compiling the model with binary cross-entropy loss, Adam optimizer, and accuracy metric
model_1.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=['accuracy']
)

# Training the first model for 100 epochs
history_1 = model_1.fit(x_train, y_train, epochs=100, verbose=0)

# Evaluating the model on the test set
res = model_1.evaluate(x_test, y_test)
model_results["model_1"] = {
    "loss": f"{res[0]}",  # Storing the loss
    "accuracy": f"{res[1]}"  # Storing the accuracy
}
model_results  # Displaying the model results

# Visualizing the decision boundaries for training and test data
from mlxtend.plotting import plot_decision_regions
plt.figure(figsize=(12, 6))

# Plotting the decision boundary for the training data
plt.subplot(1, 2, 1)
plt.title("Decision Boundary of Train Data")
plot_decision_regions(x_train, y_train, clf=model_1, legend=2)

# Plotting the decision boundary for the test data
plt.subplot(1, 2, 2)
plt.title("Decision Boundary of Test Data")
plot_decision_regions(x_test, y_test, clf=model_1, legend=2)

# Plotting the training history (loss and accuracy over epochs)
pd.DataFrame(history_1.history).plot()

# Training another model with an additional layer and higher learning rate

# Setting the random seed for reproducibility
tf.random.set_seed(42)

# Defining the second model with two hidden layers (increase model complexity)
model_2 = tf.keras.Sequential([
    tf.keras.layers.Dense(4, activation="relu"),  # First hidden layer with 4 neurons
    tf.keras.layers.Dense(4, activation="relu"),  # Second hidden layer with 4 neurons
    tf.keras.layers.Dense(1, activation="sigmoid")  # Output layer with sigmoid activation
])

# Compiling the model with a higher learning rate
model_2.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),  # Increased learning rate
    metrics=['accuracy']
)

# Training the second model for 100 epochs
history_2 = model_2.fit(x_train, y_train, epochs=100, verbose=0)

# Evaluating the second model on the test set
res = model_2.evaluate(x_test, y_test)
model_results["model_2"] = {
    "loss": f"{res[0]}",  # Storing the loss for model 2
    "accuracy": f"{res[1]}"  # Storing the accuracy for model 2
}
model_results  # Displaying the results for the second model

# Visualizing the decision boundaries for the second model
plt.figure(figsize=(12, 6))

# Plotting the decision boundary for the training data with model 2
plt.subplot(1, 2, 1)
plt.title("Decision Boundary of Train Data")
plot_decision_regions(x_train, y_train, clf=model_2, legend=2)

# Plotting the decision boundary for the test data with model 2
plt.subplot(1, 2, 2)
plt.title("Decision Boundary of Test Data")
plot_decision_regions(x_test, y_test, clf=model_2, legend=2)
