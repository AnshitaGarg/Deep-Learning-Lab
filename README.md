# Deep-Learning-Lab

# Experiment 1
In this experiment, we implement a neural network to classify the MNIST dataset using only NumPy, focusing on the fundamentals of training a neural network without relying on high-level libraries like TensorFlow or PyTorch.

# Experiment 2
This experiment explores analyzing and modeling a non-linear dataset using the scikit-learn library. It includes preprocessing, feature engineering, and applying various machine learning models to solve the classification problem.

# Experiment 3
In this experiment, we implement a Convolutional Neural Network (CNN) from scratch to classify images. The goal is to understand the inner workings of a CNN, including convolutional layers, pooling layers, and fully connected layers, using a dataset such as MNIST or CIFAR-10. We focus on the architecture design, training process, and evaluation of the model's performance.

# Experiment 4
In this experiment, is to explore text generation using Recurrent Neural Networks (RNNs) and understand the impact of different word representations:
1. One-Hot Encoding
2. Trainable Word Embeddings

# Experiment 5
To implement a Sequence-to-Sequence (Seq2Seq) model for English-to-Spanish translation using LSTM networks. This experiment explores two major architectures:
1.      LSTM Encoder-Decoder without Attention
2.      LSTM Encoder-Decoder with Attention:
Â·         Bahdanau (Additive) Attention
 Luong (Multiplicative) Attention
Each model is evaluated using BLEU scores and visualizations on the English-Spanish Dataset.
